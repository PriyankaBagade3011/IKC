{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import os, glob, imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.kernel_image_pair import KernelImagePair, default_augmentations, default_transforms\n",
    "from network.sftmd import SFTMD, Predictor, Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from torchvision.utils import make_grid\n",
    "def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n",
    "    '''\n",
    "    Converts a torch Tensor into an image Numpy array\n",
    "    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n",
    "    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n",
    "    '''\n",
    "    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # clamp\n",
    "    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n",
    "    n_dim = tensor.dim()\n",
    "    if n_dim == 4:\n",
    "        n_img = len(tensor)\n",
    "        img_np = make_grid(tensor, padding=0, nrow=1, normalize=False).numpy()\n",
    "        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n",
    "    elif n_dim == 3:\n",
    "        img_np = tensor.numpy()\n",
    "        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n",
    "    elif n_dim == 2:\n",
    "        img_np = tensor.numpy()\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}'.format(n_dim))\n",
    "    if out_type == np.uint8:\n",
    "        img_np = (img_np * 255.0).round()\n",
    "        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n",
    "    return img_np.astype(out_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = glob.glob(\"../data/DIV2K/DIV2K_train_HR/**/*.png\", recursive=True)\n",
    "test_imgs = glob.glob(\"../data/DIV2K/DIV2K_valid_HR/**/*.png\", recursive=True)\n",
    "scale=4\n",
    "ker_dim=10\n",
    "aniso=False\n",
    "diverse=False\n",
    "train_kernel = f\"kernels/train/kernel_scale{scale}_{'aniso' if aniso else 'iso'}_dim{ker_dim}{'_diverse' if diverse else ''}.pth\"\n",
    "test_kernel = f\"kernels/test_2/kernel_scale{scale}_{'aniso' if aniso else 'iso'}_dim{ker_dim}{'_diverse' if diverse else ''}.pth\"\n",
    "\n",
    "\n",
    "testsets = dict(\n",
    "    set5=glob.glob(\"../data/testing_datasets/Set5/*.png\", recursive=True),\n",
    "    div2k=glob.glob(\"../data/DIV2K/DIV2K_valid_HR/*.png\", recursive=True),\n",
    "#     set14=glob.glob(\"../data/testing_datasets/Set14/*.png\", recursive=True),\n",
    "    set14=glob.glob(\"../data/set14/Set14/images/*.png\", recursive=True),\n",
    "    urban100=glob.glob(\"../data/testing_datasets/Urban100/*.png\", recursive=True),\n",
    "    ffhq=glob.glob(\"../../ffhq-dataset/data/images1024x1024_test/69000/*.png\", recursive=True)\n",
    ")\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KernelImagePair(imgs=testsets['set5'], \n",
    "                            kernel_pickle=test_kernel, scale=scale, \n",
    "                            augmentations=default_augmentations, transforms=default_transforms, \n",
    "                            seed=0, train=False, cubic=True, interpolation=\"nearest\")\n",
    "dl = DataLoader(dataset)\n",
    "def image_to_bd(img, preserve_lr=False, cuda=True):\n",
    "    # img : tensor(CHW), np.array(HWC) \n",
    "    dataset.imgs = [img]\n",
    "    ds = next(iter(dl))\n",
    "    if preserve_lr:\n",
    "        ds['LR'] = ds['HR']\n",
    "    if cuda:\n",
    "        with torch.no_grad():\n",
    "            for k, v in ds.items():\n",
    "                ds[k] = ds[k].cuda()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(est, gt, is_rgb=True):\n",
    "    diff = est - gt\n",
    "    shave = scale\n",
    "    if diff.size(1) > 1 and not is_rgb:\n",
    "        gray_coeffs = [65.738, 129.057, 25.064]\n",
    "        convert = diff.new_tensor(gray_coeffs).view(1, 3, 1, 1) / 256\n",
    "        diff = diff.mul(convert).sum(dim=1)\n",
    "    valid = diff[..., shave:-shave, shave:-shave]\n",
    "    mse = valid.pow(2).mean()\n",
    "    # 20 * torch.log10(1/(diff))\n",
    "    return -10 * torch.log10(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr =  '/home/choijongho/workspace/sang/ikc_synthetic/butterfly.png'\n",
    "hr = '/home/choijongho/synthetic_lrs/original/butterfly.png'\n",
    "def psnr_img_path(sr, hr):\n",
    "    sr = imageio.imread(sr)\n",
    "    sr = sr.transpose((2, 0, 1))\n",
    "    sr = torch.from_numpy(sr).unsqueeze(0).float() / 255\n",
    "    hr = imageio.imread(hr)\n",
    "    hr = hr.transpose((2, 0, 1))\n",
    "    hr = torch.from_numpy(hr).unsqueeze(0).float() / 255\n",
    "    return psnr(sr, hr, is_rgb=False).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.72319221496582\n",
      "22.68343162536621\n"
     ]
    }
   ],
   "source": [
    "sr_edsr =  '/home/choijongho/workspace/sang/synthetic_set14/img_001_SRF_4_HR_x4_SR.png'\n",
    "sr_ikc = '/home/choijongho/workspace/ikc/set14/img_001_SRF_4_HR.png'\n",
    "hr = \"/home/choijongho/workspace/sisr/data/set14/Set14/images/img_001_SRF_4_HR.png\"\n",
    "print(psnr_img_path(sr_edsr, hr))\n",
    "print(psnr_img_path(sr_ikc, hr))\n",
    "# sr =  '/home/choijongho/workspace/ikc/butterfly.png'\n",
    "# print(psnr_img_path(sr, hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.2880)\n",
      "[tensor(33.7877), tensor(35.1845), tensor(28.7711), tensor(33.0057), tensor(30.6908)]\n"
     ]
    }
   ],
   "source": [
    "srs = sorted(glob.glob(\"/home/choijongho/test/sr/*.png\"))\n",
    "hrs = sorted(glob.glob(\"/home/choijongho/test/hr/*.png\"))\n",
    "\n",
    "psnrs = []\n",
    "for i in range(5):\n",
    "    sr = imageio.imread(srs[i])\n",
    "    sr = sr.transpose((2, 0, 1))\n",
    "    sr = torch.from_numpy(sr).unsqueeze(0).float() / 255\n",
    "    hr = imageio.imread(hrs[i])\n",
    "    hr = hr.transpose((2, 0, 1))\n",
    "    hr = torch.from_numpy(hr).unsqueeze(0).float() / 255\n",
    "    psnrs.append(psnr(sr, hr, is_rgb=False))\n",
    "    \n",
    "print(sum(psnrs) / 5)\n",
    "print(psnrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_LR(img, seed=None):\n",
    "    if seed is not None:\n",
    "        dataset.random = np.random.RandomState(seed)\n",
    "    bd = image_to_bd(img)\n",
    "    lr = Image.fromarray(tensor2img(bd['LR']))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(imgs, destination=\"test_imgs/\"):\n",
    "    os.makedirs(destination + \"/lr/\", exist_ok=True)\n",
    "    os.makedirs(destination + \"/hr/\", exist_ok=True)\n",
    "    os.makedirs(destination + \"/k/\", exist_ok=True)\n",
    "    dataset.random = np.random.RandomState(1)\n",
    "    for img in imgs:\n",
    "        bd = image_to_bd(img, cuda=False)\n",
    "        lr = Image.fromarray(tensor2img(bd['LR']))\n",
    "        lr.save(os.path.join(destination + \"/lr/\", os.path.basename(img)))\n",
    "        pil = Image.open(img)\n",
    "        pil.save(os.path.join(destination + \"/hr/\", os.path.basename(img)))\n",
    "        k = bd['k'][0]\n",
    "\n",
    "#         plt.imshow(k.numpy())\n",
    "#         plt.show()\n",
    "        torch.save(k, os.path.join(destination + \"/k/\", os.path.basename(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(testsets['set5'], \"test_imgs/set5/iso/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f44fa620b38>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = torch.load(\"test_imgs/set5/iso/k/butterfly.png\")\n",
    "plt.add_sub\n",
    "plt.imshow(k.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sisr] *",
   "language": "python",
   "name": "conda-env-sisr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
